{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t09eeeR5prIJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "GCCk8_dHpuNf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# Génération de texte avec un RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwpJ5IffzRG6"
   },
   "source": [
    "Ce tutoriel explique comment générer du texte à l'aide d'un RNN basé sur des caractères. Nous allons travailler avec un ensemble de données de l'écriture de Molière tirée de Project Gutenberg (http://www.gutenberg.org). Étant donné une séquence de caractères, entraînez un modèle pour prédire le prochain caractère de la séquence. Par exemple, si nous sommes donnés la séquence de caractères \"Molièr\", prédire le prochain caractère tel que \"e\". Des séquences de texte plus longues peuvent être générées en appelant le modèle à plusieurs reprises.\n",
    "\n",
    "Activez l'accélération GPU pour exécuter ce notebook plus rapidement. Dans Colab: *Runtime > Change runtime type > Hardware acclerator > GPU*.\n",
    "\n",
    "Ce tutoriel comprend du code de [tf.keras] (https://www.tensorflow.org/programmers_guide/keras) et de [eager execution] (https://www.tensorflow.org/programmers_guide/eager). Voici un exemple de sortie lorsque le modèle de ce tutoriel a été entraîné pour 30 époques.\n",
    "\n",
    "<pre>\n",
    "    DON PÈDRE.\n",
    "\n",
    "    Il est vrai, ma fille, vous représentez un peu la conscience,\n",
    "      Gardez qu'on accommoder sur ce moment comme lui.\n",
    "\n",
    "        SOSIE.\n",
    "\n",
    "                  Me voilà donc, monsieur, d'une âme:\n",
    "        Tous deux également tendent toute l'amour que cela pourra done.\n",
    "\n",
    "    SCÈNE III.--DON JUAN, SGANARELLE.\n",
    "\n",
    "    DON JUAN.\n",
    "\n",
    "    Me fera-t-on l'âme te fait? J'ai voulu se tacevoir tout nu, puisque tu puis voir accepter votre petits vertus,\n",
    "    Qui me fait souffrir qu'un tour exemplaisir;\n",
    "      Et ce choix démentir maître un certe montsir Oronte, Dupa chose;\n",
    "    Et, tandis qu'à cause les plus grands médecins tous les diables. Et mous offrir médecins et indivertirs son\n",
    "    nennence de sa façon.\n",
    "\n",
    "  MARPHURIUS.\n",
    "\n",
    "  Assurément; mais mon cher marquis, je ne l'équez vous plaire;\n",
    "    Ces grands satyres affreux des chanses le plus vite que vous avez\n",
    "    que je me sers avec grands merveilles; et c'est là votre personne en mêle,\n",
    "    Des lui soient prévenus de la beauté du ciel;\n",
    "     Dander de la belle parole; je suis assez humani.\n",
    "    Je serai cette prière raison d'être médecine?\n",
    "\n",
    "    SGANARELLE.\n",
    "\n",
    "    Ce vie embarrasser qu'on n\n",
    "</pre>\n",
    "\n",
    "Bien que certaines phrases soient grammaticales, la plupart n’ont pas de sens. Le modèle n'a pas appris le sens des mots, mais considérez:\n",
    "\n",
    "* Le modèle est basé sur les caractères. Lorsque la formation a commencé, le modèle ne savait pas comment épeler un mot français, ni que ces mots constituaient même une unité de texte.\n",
    "\n",
    "* La structure de la sortie ressemble aux blocs de texte dans une pièce commencent généralement par un nom de locuteur, en toutes lettres majuscules, similaire à l'ensemble de données.\n",
    "\n",
    "* Comme illustré ci-dessous, le modèle est entraîné sur les petites séquences de texte (100 caractères chacun), mais est capable de générer une séquence de texte plus longue avec une structure cohérente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## Préparatifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Importer TensorFlow et d'autres bibliotheques (libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vous pouvez utiliser le code au-dessus si vous exécutez votre programme sur colab pour copier les fichiers de données\n",
    "# à l'environment locale de colab.\n",
    "#from google.colab import files\n",
    "#!mkdir data\n",
    "#%cd data\n",
    "#upload = files.upload()\n",
    "#%cd ..\n",
    "\n",
    "try:\n",
    "  tf.enable_eager_execution()\n",
    "except Exception:\n",
    "  pass\n",
    "if tf.executing_eagerly() :\n",
    "    print(\"Eager execution enabled\")\n",
    "else:\n",
    "    print(\"Eager execution enabled failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### Telecharger les données de Molière\n",
    "\n",
    "Vous pouvez modifier la ligne suivante pour exécuter ce code sur vos propres données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "# lire le fichier local\n",
    "# il y a 5 fichiers d'ensembles de textes de Molière\n",
    "# molière1.txt 465Ko, molière2.txt 448Ko, molière3.txt 513Ko, molière4.txt 472Ko, molière50000.txt 1436Ko, molière.txt 2032Ko\n",
    "# molière.txt est l'ensemble de tous les oeuvres de Molière, molière1-4 est l'ensemble divisié à travers 4 fichiers, et\n",
    "# molière50000.txt est la moitié de l'ensemble complete.\n",
    "# il y a aussi une sélection de Shakespeare en anglais, shakespeare.txt 1090Ko\n",
    "# Vous pouvez tester sur votre ordinateur avec les fichiers plus petits avant de passer aux fichiers plus grands sur colab.\n",
    "path_to_file = 'data/molière50000.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Lire les données\n",
    "\n",
    "Jetez un coup d'œil sur le texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavnuByVymwK"
   },
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "outputs": [],
   "source": [
    "# Regardez les premièrs 500 caractères dans le texte\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tâche 1 (Caractères Features)** Selectionnez et triez les features (caractères uniques)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [],
   "source": [
    "# trouvez tous les caractères uniques dans le texte\n",
    "vocab = #######  Votre code ici #######\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Traiter le texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectoriser le texte\n",
    "\n",
    "Afin de préparer pour l'étape d'entraînement, nous devons convertir les chaînes de caractères à une représentation numérique. Créez deux lookup tables (table de correspondance): une mappant des caractères aux nombres et une autre pour des nombres aux caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tâche 2 (Préparez le texte)**  Convertissez les caractères du texte en valeurs entières."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array( ####### Votre code ici ####### )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYyNlCNXymwY"
   },
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1VKcQHcymwb"
   },
   "outputs": [],
   "source": [
    "# Montrez comment les 13 premiers caractères du texte sont mappés à des valeurs entières\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
    "# Montrez comment les 13 derniers caractères du texte sont mappés à des valeurs entières\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[-13:]), text_as_int[-13:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### La prévision par caractère"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "Étant donné un caractère ou une séquence de caractères, quel est le caractère suivant le plus probable? C'est la tâche que nous entraînerons le modèle à effectuer. L'entrée au modèle sera une séquence de caractères, et nous apprenons au modèle à prédire la sortie - le caractère suivant à chaque pas de temps.\n",
    "\n",
    "Plus précisément pour le RNN, puisque les RNN maintiennent un état interne qui dépend des éléments vus précédemment, la tâche se propose comme : étant donné tous les caractères calculés jusqu'à ce moment, quel est le prochain caractère ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Créer des textes pour l'entraînement et ses cibles (les bonnes réponses)\n",
    "\n",
    "Divisez le texte en séquences. Chaque séquence d'entrée contiendra les caractères `seq_length` du texte.\n",
    "\n",
    "Pour chaque séquence d'entrée, les cibles correspondantes contiennent la même longueur de texte, à l'exception du caractère décalé d'un caractère vers la droite.\n",
    "\n",
    "Donc, divisez le texte en morceaux de `seq_length + 1`. Par exemple, disons `seq_length` vaut 4 et notre texte est \"Hello\". La séquence d'entrée serait \"Hell\" et la séquence cible \"ello\".\n",
    "\n",
    "D’abord, nous utiliserons la fonction `tf.data.Dataset.from_tensor_slices` pour convertir le vecteur de texte en un flux (stream) d’index de caractères."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tâche 3 (Datasets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "# **seq_length** La longueur maximale en caractères de la phrase pour une seule entrée\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Créez morceaux de texte pour l'entraînement et les cibles\n",
    "char_dataset = ### VOTRE CODE ICI Créez le Dataset char_dataset des tensor slices (le texte vectorisé) ###\n",
    "\n",
    "# .Numpy peut être utiliser uniquement en mode eager. En mode graphique, vous devez utiliser\n",
    "# eval dans une session pour obtenir la valeur du tenseur dans le tableau numpy.\n",
    "if tf.executing_eagerly() :\n",
    "    for i ### VOTRE CODE ICI Imprimez les premiers 5 éléments du Dataset pour l'entraînement ###     \n",
    "        print(idx2char[i.numpy()])\n",
    "else :\n",
    "    iter = char_dataset.make_one_shot_iterator()\n",
    "    with tf.Session() as sess:\n",
    "        for x in range(0,5):\n",
    "            print(idx2char[(sess.run(iter.get_next()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "La méthode `batch` combine des éléments consécutifs d'un Dataset en lots (batch). Cela nous permet de convertir facilement des caractères individuels en séquences de la taille souhaitée **seq_length+1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4hkDU3i7ozi"
   },
   "outputs": [],
   "source": [
    "# sequences = ### VOTRE CODE ICI Combine des éléments (batch) du Dataset d'entraînement. Utilisez la taille seq_length+1 et\n",
    "# assurez que tous les batch (le dernier compris) ont la même taille. ###\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item ### VOTRE CODE ICI Imprimez les premières 5 séquences ###\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "**map** exécute des fonctions sur les éléments de le dataset et renvoie un nouvel dataset contenant les éléments transformés, dans le même ordre de l'entrée.\n",
    "Pour chaque séquence, dupliquez-la et décalez-la d'un caractère pour créer le texte d'entrée et le texte cible en utilisant la méthode `map` pour appliquer l'objet split_input_target aux séquences déjà découpées en tranches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tâche 3** Créez le texte d'entrée et son texte cible (la bonne réponse), les données d'entraînement (train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = ####### Votre code ici #######\n",
    "    target_text = ####### Votre code ici ####### \n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = ### VOTRE CODE ICI Utilisez le method map pour exécuter la fonction split_input_target afin de transformer \n",
    "                                                    # les séquences en nouvelles séquences décaler par un caractère"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiCopyGZymwi"
   },
   "source": [
    "**Tâche 4 Compréhension**\n",
    "Imprimer la première séquence : le texte d'entrée et le texte ciblé. Assurez-vous que vous comprenez au-dessous comment la fonction split_input_target est exécutée sur le premier élément du Dataset dataset afin de produire les deux chaînes de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_33OHL3b84i0"
   },
   "source": [
    "Chaque elément (@index) de ces vecteurs est traité comme un pas de temps. Pour l'entrée au pas de temps 0, le modèle reçoit l'index pour la toute première lettre \"F\" et essaie de prédire l'indice pour \"i\" en tant que caractère suivant. Au pas de temps suivant, il fait la même chose mais le `RNN` considère le contexte de l’étape précédente en plus du caractère d’entrée actuel.\n",
    "\n",
    "Nous imprimons les premiers 5 étapes avec la lettre d'entrée (pas de temp n) et la lettre prédite (pas de temp n+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eBu9WZG84i0"
   },
   "outputs": [],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Créer des lots (batch) d'entraînement\n",
    "\n",
    "Utiliser `tf.data` pour scinder le texte en séquences gérables (batch). La taille des lots (BATCH_SIZE) est une paramètre relative à la performance, typiquement reposant sur l'expérience. Puis, nous allons mélanger (shuffle) les lots. En mélangeant les lots, on essaye de rendre l'entraînement plus robuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tâche 5** Scindez (batch), puis, mélangez (shuffle) les lots. Utilisez les methodes de tf.data.Dataset. Fixez la paramètre\n",
    "drop_remainder=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [],
   "source": [
    "# Taille de lots en octets\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Taille du tampon utilisé pour mélanger les lots en octets\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = ####### VOTRE CODE ICI #######\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Construire le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "**Tâche 6** Ecrivez le code pour le fonction build_model en bas.\n",
    "Utilisez `tf.keras.Sequential` pour définir le modèle. Pour notre projet, trois couches (layers) sont utilisées pour définir notre modèle:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: La couche d'entrée. Une table de consultation (lookup table) utiliser pour mapper les numéros de chaque caractère à un vecteur de dimensions `embedding_dim`;\n",
    "* `tf.keras.layers.GRU`: Un RNN de taille `units=rnn_units`\n",
    "* `tf.keras.layers.Dense`: La couche de sortie, avec `vocab_size` pour la dimension de la sortie .\n",
    "Les astuces sont données en bas pour configuer les paramètres pour chaque couche.\n",
    "\n",
    "Pour chaque caractère, le modèle trouve l'embedding, exécute GRU un seul pas de temps avec l'embedding en tant qu'entrée et applique la couche dense pour générer des logits (numéro réel) prédisant le log-likelihood (probabilité) du caractère suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Taille du vocabulaire en caractères\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# La dimension de la couche d'entrée Embedding.\n",
    "embedding_dim = 256\n",
    "\n",
    "# Nombre d'unités RNN\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "#    # Taille des séquences d'entrée\n",
    "#  Embedding_input_length = [batch_size, None]    \n",
    "#  # astuces paramétres Embedding layer\n",
    "#  #    Fixer la valeur de vocab_size, embedding_dim, et batch_input_shape=[batch_size, None]\n",
    "#  #    ne pas utiliser: embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero\n",
    "#  # astuces paramétres GRU Layer\n",
    "#  #    Fixez la valuer de rnn_units. Fixez return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'\n",
    "#  #    ne pas utiliser tous autres paramétres\n",
    "#  # astuces paramétres Dense Layer\n",
    "#  #    Fixez la valeur de units. Ne pas utiliser tous autres paramétres.\n",
    "#    model = ######## VOTRE CODE ICI #######\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Essayer le modèle (avant l'entraînement)\n",
    "\n",
    "Le modèle peut être utilisé directement sans entraînement. On ne peut pas attendre les bons résultats, mais nous pouvons vérifier que le fonctionnement du modèle.\n",
    "\n",
    "La taille de séquence de l'entrée est 100 mais le modèle peut être exécuté sur des entrées de n'importe quelle taille.\n",
    "\n",
    "Nous exécutons le model sur la première séquence dans le Dataset (qui est aléatoire parce que nous avons effectué un \"shuffle\". Puis, nous imprimons le shape (dimensions) de la sortie (les prévisions). (Pourquoi ces dimensions?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-_70kKAPrPU"
   },
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous regardons à la structure du modèle. Nous voyons les dimensions de chaque couche (pourquoi ces dimensions?) et le nombre de paramètres entraînable dans chaque couche (ce numéro dépend sur la composition interne de la couche)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "**example_batch_predictions** contient les probabilités (logits) initiales pour la chaîne de 100 caractères (_sequence_length_) dans les 64 chaînes (_batch_size_). Nous allons trouver les indices de caractères pour chaque prévision.\n",
    "\n",
    "Nous prenons les échantillons aléatoires (_random.categorical_). Ils sont les valeurs initiales pour démarrer l'entraînement.  Les valeurs sont trouvées aléatoirement et pas simplement par le _argmax_ de chaque lot (batch) afin que l'entraînement progresse bien vers un vrai minima (en raison de le comportement de gradient descent).\n",
    "\n",
    "Nous l'essayons pour le premier exemple dans les lots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(example_batch_predictions))\n",
    "print(len(example_batch_predictions[0]))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sampled_indices))\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_squeeze_ pour faire une 1D _numpy_ tableau (array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "Maintenant, pour chaque pas de temp, nous avons une prévision pour le prochain indice de caractère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfLtsP3mUhCG"
   },
   "source": [
    "Et convertir l'indice en caractère pour voir le texte prévu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWcFwPwLSo05"
   },
   "outputs": [],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Entraîner le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "Maintenant, le problème est convenablement configuré et nous pouvons le traiter comme une classification normale. Donnant l'état précédente du RNN, et la donnée d'entrée pour le pas de temps actuel, prédire la classe de le prochain caractère."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "### Definir une loss function (fonction de perte), Compiler le modèle avec un optimizer (optimisateur) et la loss fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAjbjY03eiQ4"
   },
   "source": [
    "Nous utiliserons la loss function `tf.keras.losses.sparse_categorical_crossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeOXriLcymww"
   },
   "source": [
    "**Tâche 7** Compilez les paramètres de l'entraînement avec `tf.keras.Model.compile` . Nous emploierons juste deux paramètres, la méthode de réviser les parametres (optimizer) et la méthode pour calculer la fonction de perte (loss). L'optimizer recommandé est 'adam' (`tf.keras.optimizers.Adam`). L'optimisation d'Adam est une méthode de descente de gradient stochastique (stochastic gradient descent) basée sur une estimation adaptative des moments du premier et du second ordre. La fonction de perte est la fonction que vous avez créée et testée au-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "####### VOTRE CODE ICI #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configuer les checkpoints (pointes de contrôles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XBUUavgF56"
   },
   "source": [
    "Nous allons sauvegarder les checkpoints (pointes de contrôle) pour chaque epoch avec les poids (weights) que le modèle a trouvé. Une fois que les checkpoint sont sauvegardés, nous pouvons revenir à un checkpoint pour générer les prévisions avec les poids qui nous avons obtenu à ce niveau d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Dosser où les checkpoints seront sauvegardés\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Nom de chaque fichier checkpoint\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Executer l'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "Il faut maintenant choisir le nombre d'époques (EPOCHS) pour l'entraînement. Un fichier de 1500 Ko (molière5000) colab, sans GPU, a pris 1255 seconds par époque. Nous pouvons demander d'utiliser le GPU si nous faisons l'entraînement sur Colab pour raccourcir le temps - mais, si un GPU n'est pas disponible le code exécutera sur un CPU. Sur un fichier de 1000 Ko (shakespeare) mon ordinateur avec un Ryzen 5 chaque époque a pris 20 minutes, sur le GPU de Colab, 24 secondes. Le CPU de Colab (sans accélération) a pris 16 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS= ### VOTRE VALEUR ICI ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll"
   },
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Génération du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "### Restauer le dernier checkpoint (point de contrôle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyeYRiuVjodY"
   },
   "source": [
    "Pour l'inférence (utilisation d'un modèle entraîné pour les prévisions) nous pouvons traiter les données avec un taille de lot (_batch_size_) = 1. Mais, il faut reconstruire (_build_) le modèle avec la nouvelle _batch_size_. On a besoin juste de lui fournir les poids (_weights_) finals, pas besoin de répéter l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk2WJ2-XjkGz"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xa6jnYVrAN"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### La boucle de prévision\n",
    "\n",
    "La boucle suivante de code générera le texte:\n",
    "\n",
    "* Initialisation: une chaîne de caractères de début, l'état du RNN et le nombre de caractères à générer.\n",
    "\n",
    "* Obtenez la distribution de prédiction du prochain caractère avec la chaîne de caractère de début et de l'état du RNN.\n",
    "\n",
    "* Ensuite, utilisez la distribution catégorique pour calculer l’indice du caractère prédit. Puis, utilisez ce caractère prédit comme la prochaine entrée dans le modèle.\n",
    "\n",
    "* L'état du RNN renvoyé par le modèle rentre encore dans le modèle afin qu'il ait maintenant plus de contexte, au lieu d'une seule caractère. Après avoir prédit le mot suivant, les états des RNN modifiés sont à nouveau rentrés dans le modèle, ainsi le modèle apprend au fur et à mesure qu'il récupère le contexte des mots prédits précédemment.\n",
    "\n",
    "En regardant le texte généré, vous verrez que le modèle sait faire les lettres majuscules, faire des paragraphes et imiter un vocabulaire d'écriture semblable à celui de Shakespeare. Avec le petit nombre d’époques de formation, il n’a pas encore appris à former des phrases cohérentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "# L'étape de l'evaluation (génération du text par un modèle entraîné)\n",
    "\n",
    "  # Nombre de caractères à générer\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Convertir la chaîne de caractères de début aux nombres (vectorisation)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Chaîne de caractères pour le résultat\n",
    "  text_generated = []\n",
    "\n",
    "    # Les températures basses donnent un texte plus prévisible.\n",
    "    # Des températures plus élevées produisent un texte plus surprenant.\n",
    "    # Faites des essais pour trouver le meilleur réglage.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # enlever une dimension batch (squeeze)\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # on utilise la distribution catagorique pour prédire le texte renvoyé par le modèle\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # la lettre prédit est ajouté à la prochaine entrée du modèle  \n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktovv0RFhrkn"
   },
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"  GÉRONTE.: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "**ESSAYER D'AMELIORER VOS RESULTATS**\n",
    "La méthode la plus facile à améliorer les résultats est d'augmenter le nombre d'époques d'entraînement (essayez `EPOCHS=30`).\n",
    "\n",
    "Vous pouvez également expérimenter avec une autre chaîne de caractères de départ ou essayer d'ajouter une autre couche RNN pour améliorer la précision du modèle ou d'ajuster le paramètre de température pour générer des prédictions plus ou moins aléatoires."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
