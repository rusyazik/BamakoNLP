{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T18:16:27.608310Z",
     "start_time": "2017-11-05T18:16:26.423528Z"
    }
   },
   "outputs": [],
   "source": [
    "anglais = \"Two things are infinite: the universe and human stupidity; and I'm not sure about the universe\"\n",
    "francais = \"Il n'y a d'autre enfer pour l'homme que la bêtise ou la méchanceté de ses semblables.\"\n",
    "rap = \"Aujourd'hui, t'es tout mais demain t'es rien. La vie qu't'as choisie n'est p't-être pas la bonne.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T18:16:27.633134Z",
     "start_time": "2017-11-05T18:16:27.610910Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two',\n",
       " 'things',\n",
       " 'are',\n",
       " 'infinite:',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'and',\n",
       " 'human',\n",
       " 'stupidity;',\n",
       " 'and',\n",
       " \"I'm\",\n",
       " 'not',\n",
       " 'sure',\n",
       " 'about',\n",
       " 'the',\n",
       " 'universe']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(anglais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Il',\n",
       " \"n'y\",\n",
       " 'a',\n",
       " \"d'autre\",\n",
       " 'enfer',\n",
       " 'pour',\n",
       " \"l'homme\",\n",
       " 'que',\n",
       " 'la',\n",
       " 'bêtise',\n",
       " 'ou',\n",
       " 'la',\n",
       " 'méchanceté',\n",
       " 'de',\n",
       " 'ses',\n",
       " 'semblables.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(francais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Aujourd'hui,\",\n",
       " \"t'es\",\n",
       " 'tout',\n",
       " 'mais',\n",
       " 'demain',\n",
       " \"t'es\",\n",
       " 'rien.',\n",
       " 'La',\n",
       " 'vie',\n",
       " \"qu't'as\",\n",
       " 'choisie',\n",
       " \"n'est\",\n",
       " \"p't-être\",\n",
       " 'pas',\n",
       " 'la',\n",
       " 'bonne.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(rap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two',\n",
       " 'things',\n",
       " 'are',\n",
       " 'infinite',\n",
       " ':',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'and',\n",
       " 'human',\n",
       " 'stupidity',\n",
       " ';',\n",
       " 'and',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'about',\n",
       " 'the',\n",
       " 'universe']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(anglais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T18:16:27.647746Z",
     "start_time": "2017-11-05T18:16:27.637909Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two',\n",
       " 'things',\n",
       " 'are',\n",
       " 'infinite',\n",
       " ':',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'and',\n",
       " 'human',\n",
       " 'stupidity',\n",
       " ';',\n",
       " 'and',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'not',\n",
       " 'sure',\n",
       " 'about',\n",
       " 'the',\n",
       " 'universe']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(anglais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
